{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "<br>\n",
    "Notebook for evaluating models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91985267f07d2d2f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.utils import *\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_curve, auc"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-08-06T12:38:51.570652Z"
    }
   },
   "id": "f785e0f5498b313f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "probs_resnet_clean_csv = '../eval/prob_resnet_spec_eval.csv'\n",
    "probs_LCNN_clean_csv = '../eval/prob_LCNN_spec_eval.csv'\n",
    "probs_SENet_clean_csv = '../eval/prob_SENet_spec_eval.csv'\n",
    "\n",
    "probs_resnet_clean = pd.read_csv(probs_resnet_clean_csv, header=0, engine='python')\n",
    "probs_LCNN_clean = pd.read_csv(probs_LCNN_clean_csv, header=0, engine='python')\n",
    "probs_SENet_clean = pd.read_csv(probs_SENet_clean_csv, header=0, engine='python')\n",
    "\n",
    "if len(probs_resnet_clean) == len(probs_LCNN_clean) == len(probs_SENet_clean):\n",
    "    print(f'OK. Lengths are the same: {len(probs_resnet_clean)-1}')\n",
    "else:\n",
    "    sys.exit('Not OK. Lengths are not the same')\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "aa113646bf9daa7a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# ground truth labels of the evaluation dataset (ASVSpoof2019)\n",
    "config_path_resnet = '../config/residualnet_train_config.yaml'\n",
    "config_resnet = read_yaml(config_path_resnet)\n",
    "df_eval = pd.read_csv(os.path.join('..', config_resnet['df_eval_path']))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a2d10fafabd060d4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## Extract predicted labels in the same order as in df_eval_19"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34a1632a2ee96a7e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_id(file_path):\n",
    "    match = re.search(r'LA_E_(\\d+)', file_path)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "    \n",
    "    \n",
    "def pred_probabilities(file2_path):\n",
    "    # read df_eval_19\n",
    "    file1_path = '../data/df_eval_19.csv'\n",
    "    \n",
    "    file1_ids = []\n",
    "    with open(file1_path, 'r') as file1:\n",
    "        csv_reader = csv.reader(file1)\n",
    "        for row in csv_reader:\n",
    "            file_id = extract_id(row[1])\n",
    "            if file_id:\n",
    "                file1_ids.append(file_id)\n",
    "    \n",
    "    # read second file and store data in a dictionary\n",
    "    file2_data = {}\n",
    "    with open(file2_path, 'r') as file2:\n",
    "        csv_reader = csv.reader(file2)\n",
    "        for row in csv_reader:\n",
    "            file_id = extract_id(row[0])\n",
    "            if file_id:\n",
    "                file2_data[file_id] = (float(row[1]), float(row[2]))\n",
    "                \n",
    "    output_array = []\n",
    "    for file_id in file1_ids:\n",
    "        if file_id in file2_data:\n",
    "            col2, col3 = file2_data[file_id]\n",
    "            output_array.append(0 if col2>col3 else 1)\n",
    "                \n",
    "    return output_array"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1f35687f4c947307",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## Accuracies on clean dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae114b5dcb6d0705"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pred_labels_clean_resnet = pred_probabilities(file2_path=probs_resnet_clean_csv)\n",
    "pred_labels_clean_LCNN = pred_probabilities(file2_path=probs_LCNN_clean_csv)\n",
    "pred_labels_clean_SeNet = pred_probabilities(file2_path=probs_SENet_clean_csv)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e203e67c21a44914",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "GT_labels = df_eval.iloc[:,-1].tolist()\n",
    "print(len(GT_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "488ef69172e97a7c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## Unbalanced accuracies on clean dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd221bb14da905aa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "UA_resnet_clean = accuracy_score(y_true=GT_labels, y_pred=pred_labels_clean_resnet)\n",
    "UA_LCNN_clean = accuracy_score(y_true=GT_labels, y_pred=pred_labels_clean_LCNN)\n",
    "UA_SeNet_clean = accuracy_score(y_true=GT_labels, y_pred=pred_labels_clean_SeNet)\n",
    "\n",
    "print(f'The unb.acc. for clean dataset for ResNet is {UA_resnet_clean*100:.2f}%\\n'\n",
    "      f'The unb.acc. for clean dataset for LCNN is {UA_LCNN_clean*100:.2f}%\\n'\n",
    "      f'The unb.acc. for clean dataset for SENet is {UA_SeNet_clean*100:.2f}%\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "99e1d3ec2f212709",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## Balanced accuracy on clean dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56f584d7f2e5b12d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BA_resnet_clean = balanced_accuracy_score(y_true=GT_labels, y_pred=pred_labels_clean_resnet)\n",
    "BA_LCNN_clean = balanced_accuracy_score(y_true=GT_labels, y_pred=pred_labels_clean_LCNN)\n",
    "BA_SeNet_clean = balanced_accuracy_score(y_true=GT_labels, y_pred=pred_labels_clean_SeNet)\n",
    "\n",
    "print(f'The bal.acc. for clean dataset for ResNet is {BA_resnet_clean*100:.2f}%\\n'\n",
    "      f'The bal.acc. for clean dataset for LCNN is {BA_LCNN_clean*100:.2f}%\\n'\n",
    "      f'The bal.acc. for clean dataset for SENet is {BA_SeNet_clean*100:.2f}%\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6c3d4a9067117ae0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "***\n",
    "## Balanced accuracies of LCNN on FGSM on ResNet \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90e80f6191fb959f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_and_print_bal_accuracy(values, model, attack, at_model):\n",
    "    for i in values:\n",
    "        epsilon_str = str(i).replace('.', 'dot')\n",
    "        csv_file = f'../eval/prob_{model}_{attack}_{at_model}_{epsilon_str}.csv'\n",
    "        #csv = pd.read_csv(csv_file, header=0, engine='python')\n",
    "        probs = pred_probabilities(file2_path=csv_file)\n",
    "        BA = balanced_accuracy_score(y_true=GT_labels, y_pred=probs)\n",
    "        \n",
    "        print(f'Balanced accuracy for {model} on {attack} {at_model} for eps={i} is {BA*100:.2f}%')\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1dd8e9a4980c7c02",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "compute_and_print_bal_accuracy([0.2, 0.4, 0.6, 0.8, 1.0, 2.0, 3.0], 'LCNN', 'FGSM', 'ResNet')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ee58031fd00288df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "782f78098db8ce4c",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
